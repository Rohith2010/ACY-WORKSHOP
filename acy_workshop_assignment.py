# -*- coding: utf-8 -*-
"""ACY WORKSHOP ASSIGNMENT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VUGYfaG3dxNIrCpjoovDxamSCmQR1hcV

# Importing libraries
"""

from imblearn.over_sampling import RandomOverSampler, ADASYN
from collections import Counter

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split, KFold, GridSearchCV, ParameterGrid, cross_val_score, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.preprocessing import MinMaxScaler, PowerTransformer, StandardScaler
from sklearn.metrics import roc_auc_score
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
from imblearn.over_sampling import SMOTE
from sklearn.svm import SVC
from sklearn import metrics




import warnings
warnings.filterwarnings("ignore")

"""# Reading Data"""

df = pd.read_csv("/content/creditcard.csv")

"""# Initial Analysis"""

df.head()

df.describe()

df.info()

"""### Findings
1. There are no missing values
2. There are 30 independent variables
3. Standardization required as all the variables are on different scale

# Exploratory Data Analysis

## Correlation
"""

# Creating a correlation map
mask = np.triu(df.corr())
plt.figure(figsize=(25, 15), dpi=200)
sns.heatmap(df.corr(),
           vmin=-1,
           vmax=1,
           cmap='coolwarm',
           annot=True,
           fmt='.2f',
           mask=mask)
plt.show(block=False)

"""### Here we will observe the distribution of our classes"""

classes=df['Class'].value_counts()
normal_share=classes[0]/df['Class'].count()*100
fraud_share=classes[1]/df['Class'].count()*100

classes



"""## Relation between Time and Amount"""

df['TimeMin'] = df['Time'] / 60
df['TimeHour'] = df['Time'] / 60**2

plt.figure(figsize=(20, 5), dpi=150)
sns.histplot(df[df.Class==1].TimeMin, binwidth=30)
plt.xticks(range(0, 3000, 30), range(0, 3000, 30), rotation=90)
plt.show(block=False)

"""### Findings
1. Some Fraudulent transactions occur right in the first hour but the count is very low.
2. Most of the fradulent transactions happened after 660-690 mins
"""

plt.figure(figsize=(15, 5))
sns.histplot(df[df.Class==1].TimeHour, binwidth=2)
plt.xticks(range(0, 50, 2), range(0, 50, 2))
plt.show(block=False)

"""## Amount"""

df[df.Class==1].Amount.describe()

plt.figure(figsize=(15, 8))
plt.title("Fraudulent Transactions")
sns.histplot(df[df.Class==1].Amount, binwidth=50)
plt.xticks(range(0, 2500, 50), range(0, 2500, 50), rotation=90)
plt.show()

"""### Findings
1. Most of the fradulent transactions are in the range of 0-50 dollars
2. Highest Fraud transaction was at between 2100-2150 dollars

## Amount and Class
"""

sns.scatterplot(data=df, x='Class', y='Amount')
plt.show()

"""### Findings
1. Even before trying out the graph above, we already saw that the maximum fraudulent transaction was between 2100-2150 dollars
2. We can check and see the Amount column in order to constraint the datapoints to look at transactions that are worth less than 2500 dollars.

## Time and Class
"""

#sns.scatterplot(df.Time, df.Class)
sns.scatterplot(data=df, x='Time', y='Class')
plt.show()

"""### Findings
1. We can see that fradulent cases can happen between any timeframe which does not conclude any particular timeframe for a fradulent transaction. So we can remove Time variable
"""

df.drop(['Time', 'TimeMin', 'TimeHour'], axis=1, inplace=True)

"""# Train test split"""

# Train test split
y = df.pop("Class")
X = df

# Using stratify=y for splitting data into stratified fashion
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, stratify=y, random_state=42)

"""# Checking for variable distribution"""

# plot the histogram of a variable from the dataset
plt.figure(figsize=(20, 15))
for i in range(1, 30):
    plt.subplot(6, 6, i)
    sns.histplot(X_train.iloc[:, i-1])
plt.tight_layout()
plt.show(block=False)

# Skewness in data to filter the values which are beyond
# -0.5 and +0.5 to calculate skewness in a variable
skewness = df.skew()

# Subset of columns  with below condition
columns = skewness[(skewness < -0.5) | (skewness > 0.5)].index

columns

# Using PowerTransformer for Apply a power transform featurewise to make
# data more Gaussian-like for both X_train and X_test
power = PowerTransformer()
X_train[columns] = power.fit_transform(X_train[columns])
X_test[columns] = power.fit_transform(X_test[columns])

# plot the histogram of a variable from the dataset after power transformation
plt.figure(figsize=(20, 15))
for i in range(1, 30):
    plt.subplot(6, 6, i)
    sns.histplot(X_train.iloc[:, i-1])
plt.tight_layout()
plt.show(block=False)

"""# Results"""

# creating a results dataframe for later to evaluate the models
results = pd.DataFrame(columns = ['model_name', 'threshold', 'recall', 'roc_auc_score'])

"""# Model Evaluation Function"""

def model_evaluation(y_pred_proba_test):
#     Report for different thresholds
    thresholds = [i * 0.1 for i in range(0, 10)]
    print("----------------------------Results----------------------------")
#     best ROC score initialisation
    best_roc_score = 0

#     Iterating through every threshold from 0.1 to 0.9
    for threshold in thresholds:
        y_pred = np.where(y_pred_proba_test[:, 1] > threshold, 1, 0)
#   Calculating different metrics
        accuracy = str(round(metrics.accuracy_score(y_test, y_pred), 3))
        precision = str(round(metrics.precision_score(y_test, y_pred), 3))
        recall = str(round(metrics.recall_score(y_test, y_pred), 3))
        roc_auc = str(round(metrics.roc_auc_score(y_test, y_pred), 3))
#   Setting the best roc score, threshold, recall scores.
        if float(roc_auc) > best_roc_score:
            best_roc_score = float(roc_auc)
            best_threshold = threshold
            best_recall_score = recall
#   printing the results for every threshold
        print("----------for Test with threshold", round(threshold, 2), "----------")
        print("accuracy\tprecision\trecall\t\troc_auc")
        print("\t\t".join([accuracy, precision, recall, roc_auc]))
        print("\n")
#   Confusion Matrix
        print("\t\tCONFUSION MATRIX")
        confusion_matrix = pd.DataFrame(metrics.confusion_matrix(y_test, y_pred),
                                        columns=['Predicted Negative', 'Predicted Positive'],
                                        index=['Actual Negative', 'Actual Positive'])
        print(confusion_matrix)
        print("\n")
    print("BEST ROC AUC SCORE is ", best_roc_score, "at the threshold", best_threshold)
    return best_roc_score, best_threshold, best_recall_score

"""# Draw ROC"""

def draw_roc( actual, probs ):
    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,
                                              drop_intermediate = False )
    auc_score = metrics.roc_auc_score( actual, probs )
    plt.figure(figsize=(5, 5))
    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.0])
    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic')
    plt.legend(loc="lower right")
    plt.show()

"""# Logistic Regression"""

# Initial model
logit = LogisticRegression()
# Folds for crossvalidation
cv = KFold(3)
# Setting paramters
param_grid={
    'solver':['saga', 'lbfgs']
}
# Grid search for running model with every possible combimnation
# that is defined with above param_grid
grid = GridSearchCV(logit,
                   param_grid=param_grid,
                   scoring='roc_auc',
                   cv=cv,
                   n_jobs=-1,
                   verbose=1000)
grid.fit(X_train, y_train)
print("BEST GRID SCORE", grid.best_score_)
print("BEST GRID PARAMS")
print(grid.best_params_)

# Taking the best estimator and fitting data
best_logit = grid.best_estimator_
best_logit.fit(X_train, y_train)

# Taking probabilities for model evaluation with different thresholds
y_pred_proba_test = best_logit.predict_proba(X_test)
# Model Evaluation
best_roc_score, best_threshold, best_recall_score = model_evaluation(y_pred_proba_test)
# 'model_name', 'threshold', 'recall', 'roc_auc_score'
data = pd.DataFrame([['LOGISTIC REGRESSION IMB', best_threshold, best_recall_score, best_roc_score]],
                    columns=results.columns)
results = pd.concat([results,data], ignore_index=False)

draw_roc(y_test, y_pred_proba_test[:, 1])

"""# Using the same format and steps as used above for every model running to avoid redundant commenting and confusion in code

# Decision Tree
"""

dt = DecisionTreeClassifier()
cv = KFold(3, shuffle=True)
param_grid={
    'criterion':['entropy'],
    'max_depth':[5, 10],
    'min_samples_leaf':[100],
    'min_samples_split': [50]
}
grid = GridSearchCV(dt,
                   param_grid=param_grid,
                   scoring='roc_auc',
                   cv=cv,
                   n_jobs=-1,
                   verbose=1000)
grid.fit(X_train, y_train)
print("BEST GRID SCORE", grid.best_score_)
print("BEST GRID PARAMS")
print(grid.best_params_)
best_dt = grid.best_estimator_
best_dt.fit(X_train, y_train)

y_pred_proba_test = best_dt.predict_proba(X_test)

best_roc_score, best_threshold, best_recall_score = model_evaluation(y_pred_proba_test)
# 'model_name', 'threshold', 'recall', 'roc_auc_score'
data = pd.DataFrame([['DECISION TREE', best_threshold, best_recall_score, best_roc_score]],
                    columns=results.columns)
results = pd.concat([results, data], ignore_index=False)

draw_roc(y_test, y_pred_proba_test[:, 1])

# Commented out IPython magic to ensure Python compatibility.
var_imp = []
for i in best_dt.feature_importances_:
    var_imp.append(i)
print('Top var =', var_imp.index(np.sort(best_dt.feature_importances_)[-1])+1)
print('2nd Top var =', var_imp.index(np.sort(best_dt.feature_importances_)[-2])+1)
print('3rd Top var =', var_imp.index(np.sort(best_dt.feature_importances_)[-3])+1)

# Variable on Index-16 and Index-13 seems to be the top 2 variables
top_var_index = var_imp.index(np.sort(best_dt.feature_importances_)[-1])
second_top_var_index = var_imp.index(np.sort(best_dt.feature_importances_)[-2])

X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]
X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]

np.random.shuffle(X_train_0)

import matplotlib.pyplot as plt
# %matplotlib inline
plt.rcParams['figure.figsize'] = [20, 20]

plt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')
plt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],
            label='Actual Class-0 Examples')
plt.legend()

"""# Class Imbalance techniques

1. Random Oversampling

2. ADASYN
"""

oversampler = RandomOverSampler(sampling_strategy='minority')
X_over, y_over = oversampler.fit_resample(X_train, y_train)

y_over.value_counts()

"""## Logistic Regression with Balanced Dataset"""

logit = LogisticRegression()
cv = KFold(3)
param_grid={
    'solver':['saga', 'lbfgs']
}
grid = GridSearchCV(logit,
                   param_grid=param_grid,
                   scoring='roc_auc',
                   cv=cv,
                   n_jobs=-1,
                   verbose=1000)
grid.fit(X_train, y_train)
print("BEST GRID SCORE", grid.best_score_)
print("BEST GRID PARAMS")
print(grid.best_params_)
best_logit = grid.best_estimator_
best_logit.fit(X_over, y_over)

y_pred_proba_test = best_logit.predict_proba(X_test)

best_roc_score, best_threshold, best_recall_score = model_evaluation(y_pred_proba_test)
# 'model_name', 'threshold', 'recall', 'roc_auc_score'
data = pd.DataFrame([['LOGISTIC REGRESSION OVERSAMPLING', best_threshold, best_recall_score, best_roc_score]],
                    columns=results.columns)
results = pd.concat([results, data], ignore_index=False)

draw_roc(y_test, y_pred_proba_test[:, 1])

"""## Decision Tree with Balanced Dataset"""

dt = DecisionTreeClassifier()
cv = KFold(3, shuffle=True)
param_grid={
    'criterion':['entropy'],
    'max_depth':[5, 10],
    'min_samples_leaf':[50],
    'min_samples_split': [25, 50]
}
grid = GridSearchCV(dt,
                   param_grid=param_grid,
                   scoring='roc_auc',
                   cv=cv,
                   n_jobs=-1,
                   verbose=1000)
grid.fit(X_train, y_train)
print("BEST GRID SCORE", grid.best_score_)
print("BEST GRID PARAMS")
print(grid.best_params_)
best_dt = grid.best_estimator_
best_dt.fit(X_over, y_over)

y_pred_proba_test = best_dt.predict_proba(X_test)

best_roc_score, best_threshold, best_recall_score = model_evaluation(y_pred_proba_test)
# 'model_name', 'threshold', 'recall', 'roc_auc_score'
data = pd.DataFrame([['DECISION TREE OVERSAMPLING', best_threshold, best_recall_score, best_roc_score]],
                    columns=results.columns)
results = pd.concat([results, data], ignore_index=False)

draw_roc(y_test, y_pred_proba_test[:, 1])

"""# ADASYN"""

sm = ADASYN()
X_adasyn, y_adasyn = sm.fit_resample(X_train, y_train)

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings("ignore")

from imblearn import over_sampling

ada = over_sampling.ADASYN(random_state=0)
X_train_adasyn, y_train_adasyn = ada.fit_resample(X_train, y_train)
# Artificial minority samples and corresponding minority labels from ADASYN are appended
# below X_train and y_train respectively
# So to exclusively get the artificial minority samples from ADASYN, we do
X_train_adasyn_1 = X_train_adasyn[X_train.shape[0]:]

X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]
X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]



import matplotlib.pyplot as plt
# %matplotlib inline
plt.rcParams['figure.figsize'] = [20, 20]
fig = plt.figure()

plt.subplot(3, 1, 1)
plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')
plt.legend()

plt.subplot(3, 1, 2)
plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')
plt.scatter(X_train_adasyn_1.iloc[:X_train_1.shape[0], 0], X_train_adasyn_1.iloc[:X_train_1.shape[0], 1],
            label='Artificial ADASYN Class-1 Examples')
plt.legend()

plt.subplot(3, 1, 3)
plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')
plt.scatter(X_train_0[:X_train_1.shape[0], 0], X_train_0[:X_train_1.shape[0], 1], label='Actual Class-0 Examples')
plt.legend()

"""## Logistic Regression with Balanced Dataset"""

logit = LogisticRegression()
cv = KFold(3)
param_grid={
    'solver':['saga', 'lbfgs'],
    'C':[0.01, 0.1, 1, 10]
}
grid = GridSearchCV(logit,
                   param_grid=param_grid,
                   scoring='roc_auc',
                   cv=cv,
                   n_jobs=-1,
                   verbose=1000)
grid.fit(X_train, y_train)
print("BEST GRID SCORE", grid.best_score_)
print("BEST GRID PARAMS")
print(grid.best_params_)
best_logit = grid.best_estimator_
best_logit.fit(X_adasyn, y_adasyn)

y_pred_proba_test = best_logit.predict_proba(X_test)

best_roc_score, best_threshold, best_recall_score = model_evaluation(y_pred_proba_test)
# 'model_name', 'threshold', 'recall', 'roc_auc_score'
data = pd.DataFrame([['LOGISTIC REGRESSION ADASYN', best_threshold, best_recall_score, best_roc_score]],
                    columns=results.columns)
results = pd.concat([results, data], ignore_index=False)

draw_roc(y_test, y_pred_proba_test[:, 1])

"""## Decision Tree with Balanced Dataset"""

dt = DecisionTreeClassifier()
cv = KFold(3)
param_grid={
    'criterion':['entropy'],
    'max_depth':[10],
    'min_samples_leaf':[200, 300],
    'min_samples_split': [20, 50]
}
grid = GridSearchCV(dt,
                   param_grid=param_grid,
                   scoring='roc_auc',
                   cv=cv,
                   n_jobs=-1,
                   verbose=1000)
grid.fit(X_train, y_train)
print("BEST GRID SCORE", grid.best_score_)
print("BEST GRID PARAMS")
print(grid.best_params_)
best_dt = grid.best_estimator_
best_dt.fit(X_adasyn, y_adasyn)

y_pred_proba_test = best_dt.predict_proba(X_test)

best_roc_score, best_threshold, best_recall_score = model_evaluation(y_pred_proba_test)
# 'model_name', 'threshold', 'recall', 'roc_auc_score'
data = pd.DataFrame([['DECISION TREE ADASYN', best_threshold, best_recall_score, best_roc_score]],
                    columns=results.columns)
results = pd.concat([results, data], ignore_index=False)

draw_roc(y_test, y_pred_proba_test[:, 1])

# Apply SMOTE to balance the training data
smote = SMOTE(random_state=42)
X_smote, y_smote = smote.fit_resample(X_train, y_train)

# Random Forest Classifier
rf = RandomForestClassifier(random_state=42)

# Cross-validation strategy
cv = KFold(3)

# Parameter grid for Random Forest
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# GridSearchCV for hyperparameter tuning
grid = GridSearchCV(rf,
                    param_grid=param_grid,
                    scoring='roc_auc',
                    cv=cv,
                    n_jobs=-1,
                    verbose=1000)

# Fit the grid search to the SMOTE-resampled training data
grid.fit(X_smote, y_smote)

# Extract best model and performance
print("BEST GRID SCORE:", grid.best_score_)
print("BEST GRID PARAMS:")
print(grid.best_params_)
best_rf = grid.best_estimator_

# Predict probabilities on the test data
y_pred_proba_test = best_rf.predict_proba(X_test)

# Evaluate the model
best_roc_score, best_threshold, best_recall_score = model_evaluation(y_pred_proba_test)

# Log results in a DataFrame
data = pd.DataFrame([['RANDOM FOREST SMOTE', best_threshold, best_recall_score, best_roc_score]],
                    columns=results.columns)
results = pd.concat([results, data], ignore_index=False)

draw_roc(y_test, y_pred_proba_test[:, 1])

smote = SMOTE(random_state=42)
X_smote, y_smote = smote.fit_resample(X_train, y_train)

# XGBoost Classifier
xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')

# Cross-validation strategy
cv = KFold(3)

# Parameter grid for XGBoost
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

# GridSearchCV for hyperparameter tuning
grid = GridSearchCV(xgb,
                    param_grid=param_grid,
                    scoring='roc_auc',
                    cv=cv,
                    n_jobs=-1,
                    verbose=1000)

# Fit the grid search to the SMOTE-resampled training data
grid.fit(X_smote, y_smote)

# Extract best model and performance
print("BEST GRID SCORE:", grid.best_score_)
print("BEST GRID PARAMS:")
print(grid.best_params_)
best_xgb = grid.best_estimator_

# Predict probabilities on the test data
y_pred_proba_test = best_xgb.predict_proba(X_test)

# Evaluate the model
best_roc_score, best_threshold, best_recall_score = model_evaluation(y_pred_proba_test)

# Log results in a DataFrame
data = pd.DataFrame([['XGBOOST SMOTE', best_threshold, best_recall_score, best_roc_score]],
                    columns=results.columns)
results = pd.concat([results, data], ignore_index=False)

draw_roc(y_test, y_pred_proba_test[:, 1])

"""# Results DataFrame"""

results.sort_values("recall", ascending=False, inplace=True)
# results.to_csv("results.csv", index = False) #for saving the data frame to csv file
results

# Discussion for Tiny and Big Banks
def bank_recommendation(small_bank_threshold=10000):
    dataset_size = len(data)
    if dataset_size <= small_bank_threshold:
        return "Recommendation: Random Forest or Logistic Regression is suitable for tiny banks due to their simplicity and efficiency on smaller datasets."
    else:
        return "Recommendation: XGBoost is suitable for big banks due to its scalability and robustness on larger datasets."

# Print recommendation
print(bank_recommendation())

